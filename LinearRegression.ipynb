{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "线性回归在假设特证满足线性关系，根据给定的训练数据训练一个模型，并用此模型进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数（Loss Function）\n",
    "定义在单个样本上的，算的是一个样本的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数（Loss function）是用来估量你模型的预测值 $f(x)$ 与真实值 $Y$ 的不一致程度，它是一个非负实值函数，通常用 $L(Y,f(x))$来表示。损失函数越小，模型的鲁棒性（鲁棒是Robust的音译，也就是健壮和强壮的意思。它是在异常和危险情况下系统生存的关键。）就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的风险结构包括了风险项和正则项，通常如下所示：\n",
    "\n",
    "$$ \\theta^* = \\arg \\min_\\theta \\frac{1}{N}{}\\sum_{i=1}^{N} L(y_i, f(x_i; \\theta)) + \\lambda\\ J(\\theta) $$\n",
    "\n",
    "其中前面的均值函数表示的是经验风险函数，$L$代表的是损失函数，后面的 $J$ 是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是$L1$，也可以是$L2$，或者其他的正则函数。整个式子表示的意思是找到使目标函数最小时的$ \\theta $值。\n",
    "\n",
    "常见的损失误差有五种： \n",
    "1. 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中； \n",
    "2. 互熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； \n",
    "3. 平方损失（Square Loss）：主要是最小二乘法（OLS）中； \n",
    "4. 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中； \n",
    "5. 其他损失（如0-1损失，绝对值损失）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数 （Cost Function）\n",
    "定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有训练样本$(x, y)$，模型为$h$，参数为$\\theta$, 有$h(\\theta) = \\theta^Tx$ 。\n",
    "\n",
    "（1）概况来讲，任何能够衡量模型预测出来的值$h(\\theta)$与真实值$y$之间的差异的函数都可以叫做代价函数$C(\\theta)$，如果有多个样本，则可以将所有代价函数的取值求均值，记做$J(\\theta)$。因此很容易就可以得出以下关于代价函数的性质：\n",
    "\n",
    "对于每种算法来说，代价函数不是唯一的。代价函数是参数$\\theta$的函数；\n",
    "总的代价函数$J(\\theta)$可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本$(x, y)$；\n",
    "$J(\\theta)$是一个标量；\n",
    "\n",
    "（2）当我们确定了模型h，后面做的所有事情就是训练模型的参数$\\theta$。那么什么时候模型的训练才能结束呢？这时候也涉及到代价函数，由于代价函数是用来衡量模型好坏的，我们的目标当然是得到最好的模型（也就是最符合训练样本$(x, y)$的模型）。因此训练参数的过程就是不断改变$\\theta$，从而得到更小的$J(\\theta)$的过程。理想情况下，当我们取到代价函数$J$的最小值时，就得到了最优的参数$\\theta$，记为：\n",
    "\n",
    " $$\\displaystyle \\min_{ \\theta } J(\\theta)$$\n",
    "\n",
    " \n",
    "\n",
    "例如，$J(\\theta) = 0$，表示我们的模型完美的拟合了观察的数据，没有任何误差。\n",
    "\n",
    "（3）在优化参数$\\theta$的过程中，最常用的方法是梯度下降，这里的梯度就是代价函数$J(\\theta)$对$\\theta_1, \\theta_2,\\ldots,\\theta_n$的偏导数。由于需要求偏导，选择代价函数时，最好挑选对参数$\\theta$可微的函数（全微分存在，偏导数一定存在）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标函数 （Object Function）\n",
    "\n",
    "最终需要优化的函数。等于优化后的经验风险+结构风险（也就是Cost Function + 正则化项）。\n",
    "这样有如下公式：\n",
    "\n",
    "$$ \\min_\\theta \\frac{1}{N}{}\\sum_{i=1}^{N} L(y_i, f(x_i)) + \\lambda\\ J(\\theta) $$\n",
    "\n",
    "#### Tips：目标函数是最大化或者最小化，而代价函数是最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化 （Regularization）\n",
    "个人理解是对样本的某些参数进行限制以达到特定目的的处理。知乎上有人说如果将 regularizer 翻译成 “规则项” 就好理解了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合 （Overfitting）\n",
    "过拟合是指为了得到一致假设而使假设变得过度严格。\n",
    "有一个识别狗的模型，如果你的训练样本大多是一种狗，过拟合后模型会加强对这种狗的认识，就会出现这种狗的各个特征量。当你要判别另一种狗时，模型就会不认识另一种狗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 泛化能力 （Generalization Ability）\n",
    "\n",
    "是指学习到的模型对未知数据的预测能力。常用泛化误差来衡量。\n",
    "如果学习到的模型是$\\hat{f}$，那么泛化误差可表示为：\n",
    "\n",
    "$$R_e(\\hat{f}) = E_p\\left [ L(Y,\\hat{f}(X)) \\right ] = \\int L(y,\\hat{f}(x))P(x, y)dxdy$$\n",
    "\n",
    "事实上，泛化误差就是所学习到的模型的期望风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价函数（EvaluationFunction）\n",
    "预测误差的评价指标，常用的有4种：\n",
    "1. 均方根误差（RMSE）\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}{\\sum_{n}^{i=1}(Y_i - \\hat{Y_i})^2} }$$\n",
    "2. R-平方（R2）\n",
    "$$R = \\sqrt{ 1 - \\frac{\\sum_{n}^{i=1}(Y_i - \\hat{Y_i})^2}{\\sum_{n}^{i=1}(Y_i - \\bar{Y})^2} }$$\n",
    "3. 平均绝对百分误差（MAPE）\n",
    "$$MAPE = \\frac{1}{n} \\sum_{i=1}^{n}\\left | Y_i - \\hat{Y_i} \\right |$$\n",
    "4. 平均绝对误差（MAE）\n",
    "$$MAE = \\frac{\\sum_{n}^{i=1}\\left | Y_i - \\hat{Y_i}\\right|} {n}$$\n",
    "\n",
    "tips:可以调用sklearn中的函数来求得。\n",
    "\n",
    "from sklearn.metrics import mean_squared_error #均方误差\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error #平方绝对误差\n",
    "\n",
    "from sklearn.metrics import r2_score#R square\n",
    "\n",
    "mean_squared_error(y_test,y_predict)\n",
    "\n",
    "mean_absolute_error(y_test,y_predict)\n",
    "\n",
    "r2_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数求解\n",
    "\n",
    "线性回归试图学得$f(x_i) = \\omega x_i + b$,使得$f(x_i)\\approx y_i$,其中$x_i$是输入的一种属性。$y_i$为输出标记。\n",
    "### 确定$\\omega$和$b$\n",
    "$$\\left ( \\omega ^{\\ast } ,b^{\\ast }\\right ) = \\arg \\min \\sum_{i=1}^{m}\\left (f(x_i)-y_i \\right )^2 = \\arg \\min \\sum_{i=1}^{m}\\left (y_i-\\omega x_i-b \\right )^2$$\n",
    "其中$\\omega ^{\\ast } ,b^{\\ast }$表示$\\omega$和$b$的解。\n",
    "\n",
    "令$L = \\arg \\min \\sum_{i=1}^{m}\\left (y_i-\\omega x_i-b \\right )^2$\n",
    "\n",
    "分别对$\\omega$和$b$求导，则有\n",
    "$$\\frac{\\partial L}{\\partial \\omega} = 2\\left ( \\omega \\sum_{i=1}^{m}x_i^2 - \\sum_{i=1}^{m} \\left ( y_i-b \\right )x_i \\right )$$\n",
    "$$\\frac{\\partial L}{\\partial b} = 2\\left ( mb - \\sum_{i=1}^{m}\\left ( y_i - \\omega x_i \\right ) \\right )$$\n",
    "\n",
    "令$\\frac{\\partial L}{\\partial \\omega} = 0$，\n",
    "$\\frac{\\partial L}{\\partial b} = 0$。可以解得：\n",
    "\n",
    "$$\\omega = \\frac{\\sum_{i=1}^{m}y_i \\left ( x_i - \\bar{x} \\right )}{\\sum_{i=1}^{m}x_i^2 - \\frac{1}{m}\\left ( \\sum_{i=1}^{m}x_i \\right )^2}$$\n",
    "\n",
    "$$b = \\frac{1}{m}\\sum_{i=1}^{m}\\left ( y_i - \\omega x_i \\right )$$\n",
    "\n",
    "其中$\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$ 为$x$的均值。\n",
    "\n",
    "更一般的情况，有多个属性时，线性回归试图学得$f \\left (\\mathbf{x_i} \\right ) = \\pmb \\omega ^{T}\\mathbf{x_i}  + b$，使得$f\\left (\\mathbf{x_i} \\right )\\approx y_i$。\n",
    "\n",
    "此时，为了便于讨论，可以把$\\pmb \\omega$和$b$吸收入向量形式$\\hat{\\pmb \\omega}=\\left ( \\pmb \\omega;b \\right )$，同样的有$\\pmb y = \\left ( y_1;y_2;\\dots;y_m \\right )$，由此得到$$\\hat{\\pmb \\omega}^* = \\arg \\min_\\hat{\\pmb \\omega}\\left ( \\pmb y - \\mathbf{X}\\hat{\\pmb \\omega} \\right )^T \\left (  \\pmb y - \\mathbf{X} \\hat{\\pmb \\omega} \\right )$$\n",
    "\n",
    "令$L=\\left ( \\pmb y - \\mathbf{X} \\hat{\\pmb \\omega} \\right )^T \\left (  \\pmb y - \\mathbf{X} \\hat{\\pmb \\omega} \\right )$，对$\\hat{\\pmb \\omega}$求导有\n",
    "$$\\frac{\\partial L}{\\partial \\hat{\\pmb \\omega}} = 2\\mathbf{X} ^ T\\left ( \\mathbf{X} \\hat{\\pmb \\omega} - \\pmb y\\right )$$.\n",
    "\n",
    "当$\\mathbf{X}^T\\mathbf{X} $为满秩矩阵可正定矩阵时令$\\frac{\\partial L}{\\partial \\hat{\\pmb \\omega}} = 0$，可以得到$$\\hat{\\pmb \\omega}^* = \\left ( \\mathbf{X} ^ T\\mathbf{X} \\right) ^ {-1}\\mathbf{X} ^ T \\pmb y $$，其中$ \\left(\\mathbf{X} ^ T \\mathbf{X} \\right) ^ {-1} $是 $\\left(\\mathbf{X} ^ T \\mathbf{X} \\right) $的逆矩阵。\n",
    "令$ \\hat{ \\pmb {x_i}} = \\left ( \\pmb {x_i};1 \\right ) $ 则最终的线性模型为$$f\\left ( \\hat{ \\pmb {x_i}}  \\right )=  \\hat{ \\pmb {x_i}} ^{T}\\left(\\mathbf{X} ^ T \\mathbf{X} \\right) ^ {-1}\\mathbf{X} ^ T \\pmb {y}$$\n",
    "\n",
    "如果不是满秩矩阵，此时会解出多个解，需要引入正则化项。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性回归，我们希望得到的模型是预测值逼近真实值标记的线性回归模型。\n",
    "$$\\ln y = \\pmb \\omega \\pmb x ^{T} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
